<!-- doc/src/sgml/hash.sgml -->

<chapter id="hash-index">
<!-- pgdoc-cn_start sig_en=b3c994dc8134ed5131bcb3337fc14f98 sig_cn_org=None source=14.1 
<title>Hash Indexes</title>
________________________________________________________-->
<title>哈希索引</title>
<!-- pgdoc-cn_end sig_en=b3c994dc8134ed5131bcb3337fc14f98 -->

<!-- pgdoc-cn_start sig_en=4167e2a8cf84302862214790a050e6a8 sig_cn_org=None source=14.1 
   <indexterm>
    <primary>index</primary>
    <secondary>Hash</secondary>
   </indexterm>
________________________________________________________-->
   <indexterm>
    <primary>索引</primary>
    <secondary>哈希</secondary>
   </indexterm>
<!-- pgdoc-cn_end sig_en=4167e2a8cf84302862214790a050e6a8 -->

<sect1 id="hash-intro">
<!-- pgdoc-cn_start sig_en=13508788fd8dee2d1f9c928870e99338 sig_cn_org=None source=14.1 
 <title>Overview</title>
________________________________________________________-->
 <title>概述</title>
<!-- pgdoc-cn_end sig_en=13508788fd8dee2d1f9c928870e99338 -->

<!-- pgdoc-cn_start sig_en=74b77e66a579b5accc2fc43720144aad sig_cn_org=None source=14.1 
 <para>
  <productname>PostgreSQL</productname>
  includes an implementation of persistent on-disk hash indexes,
  which are fully crash recoverable. Any data type can be indexed by a
  hash index, including data types that do not have a well-defined linear
  ordering. Hash indexes store only the hash value of the data being
  indexed, thus there are no restrictions on the size of the data column
  being indexed.
 </para>
________________________________________________________-->
 <para>
  <productname>PostgreSQL</productname>
  包含永久性磁盘上的哈希索引的实现，这些索引可完全崩溃恢复。任何数据类型都可以通过哈希索引进行索引，包括没有明确定义线性排序的数据类型。哈希索引只存储被索引数据的哈希值，因此对被索引数据列的大小没有限制。
 </para>
<!-- pgdoc-cn_end sig_en=74b77e66a579b5accc2fc43720144aad -->

<!-- pgdoc-cn_start sig_en=4c4f48fe988125b68900872a8eddc4ef sig_cn_org=None source=14.1 
 <para>
  Hash indexes support only single-column indexes and do not allow
  uniqueness checking.
 </para>
________________________________________________________-->
 <para>
  哈希索引仅支持单列索引并且不允许唯一性检查。
 </para>
<!-- pgdoc-cn_end sig_en=4c4f48fe988125b68900872a8eddc4ef -->

<!-- pgdoc-cn_start sig_en=e61f7a82a95ce00ceb9a82efdb102688 sig_cn_org=None source=14.1 
 <para>
  Hash indexes support only the <literal>=</literal> operator,
  so WHERE clauses that specify range operations will not be able to take
  advantage of hash indexes.
 </para>
________________________________________________________-->
 <para>
  哈希索引仅支持<literal>=</literal>运算符，因此使用了范围操作的WHERE子句将无法利用哈希索引。
 </para>
<!-- pgdoc-cn_end sig_en=e61f7a82a95ce00ceb9a82efdb102688 -->

<!-- pgdoc-cn_start sig_en=b1ede6f5ab3ac39304ffbf0a988f9d6d sig_cn_org=None source=14.1 
 <para>
  Each hash index tuple stores just the 4-byte hash value, not the actual
  column value. As a result, hash indexes may be much smaller than B-trees
  when indexing longer data items such as UUIDs, URLs, etc. The absence of
  the column value also makes all hash index scans lossy. Hash indexes may
  take part in bitmap index scans and backward scans.
 </para>
________________________________________________________-->
 <para>
  每个哈希索引元组只存储4字节的哈希值，而不是实际的列值。因此，当索引较长的数据项（如UUID、URL等）时，哈希索引可能比B树小得多。缺少列值也会使所有哈希索引扫描有损。哈希索引可以参与位图索引扫描和反向扫描。
 </para>
<!-- pgdoc-cn_end sig_en=b1ede6f5ab3ac39304ffbf0a988f9d6d -->

<!-- pgdoc-cn_start sig_en=2533d74a051712573bb37467cb61f80d sig_cn_org=fe03a46c608637098efe7e5ad6e4f41d source=15.7 
 <para>
  Hash indexes are best optimized for SELECT and UPDATE-heavy workloads
  that use equality scans on larger tables. In a B-tree index, searches must
  descend through the tree until the leaf page is found. In tables with
  millions of rows, this descent can increase access time to data. The
  equivalent of a leaf page in a hash index is referred to as a bucket page. In
  contrast, a hash index allows accessing the bucket pages directly,
  thereby potentially reducing index access time in larger tables. This
  reduction in "logical I/O" becomes even more pronounced on indexes/data
  larger than shared_buffers/RAM.
 </para>
________________________________________________________-->
 <para>
  哈希索引最适合于对较大表进行等值扫描的SELECT和UPDATE繁重工作负载。在B树索引中，搜索必须
  通过树下降直到找到叶页。在拥有数百万行的表中，这种下降会增加对数据的访问时间。哈希索引中
  叶页的等价物被称为桶页。相比之下，哈希索引允许直接访问桶页，从而可能减少较大表中的索引访问时间。
  这种“逻辑I/O”的减少在大于shared_buffers/RAM的索引/数据上变得更加显著。
</para>
<!-- pgdoc-cn_end sig_en=2533d74a051712573bb37467cb61f80d -->

<!-- pgdoc-cn_start sig_en=6cd83952b382dccbed05be03d4fa3196 sig_cn_org=None source=14.1 
 <para>
  Hash indexes have been designed to cope with uneven distributions of
  hash values. Direct access to the bucket pages works well if the hash
  values are evenly distributed. When inserts mean that the bucket page
  becomes full, additional overflow pages are chained to that specific
  bucket page, locally expanding the storage for index tuples that match
  that hash value. When scanning a hash bucket during queries, we need to
  scan through all of the overflow pages. Thus an unbalanced hash index
  might actually be worse than a B-tree in terms of number of block
  accesses required, for some data.
 </para>
________________________________________________________-->
 <para>
  哈希索引被设计用于处理哈希值的不均匀分布。如果哈希值均匀分布，则直接访问桶页效果良好。当插入使得桶页变满时，额外的溢出页链接到该特定的桶页，本地扩展存储来容纳与该哈希值匹配的索引元组。在查询期间扫描哈希桶时，我们需要扫描所有溢出页。因此，就某些数据所需的块访问数而言，不平衡散列索引实际上可能比B树更差。
 </para>
<!-- pgdoc-cn_end sig_en=6cd83952b382dccbed05be03d4fa3196 -->

<!-- pgdoc-cn_start sig_en=a155667844b2aa2bbcf996ff91e23988 sig_cn_org=None source=14.1 
 <para>
  As a result of the overflow cases, we can say that hash indexes are
  most suitable for unique, nearly unique data or data with a low number
  of rows per hash bucket.
  One possible way to avoid problems is to exclude highly non-unique
  values from the index using a partial index condition, but this may
  not be suitable in many cases.
 </para>
________________________________________________________-->
 <para>
  通过溢出情况得到的结论是，我们可以说哈希索引最适合于唯一、几乎唯一的数据或每个哈希桶的行数较少的数据。
  避免问题的一种可能方法是使用部分索引条件从索引中排除高度非唯一的值，但这在许多情况下可能不适用。
 </para>
<!-- pgdoc-cn_end sig_en=a155667844b2aa2bbcf996ff91e23988 -->

<!-- pgdoc-cn_start sig_en=beac1daac65e217d08ab101c357863d2 sig_cn_org=None source=14.1 
 <para>
  Like B-Trees, hash indexes perform simple index tuple deletion. This
  is a deferred maintenance operation that deletes index tuples that are
  known to be safe to delete (those whose item identifier's LP_DEAD bit
  is already set). If an insert finds no space is available on a page we
  try to avoid creating a new overflow page by attempting to remove dead
  index tuples. Removal cannot occur if the page is pinned at that time.
  Deletion of dead index pointers also occurs during VACUUM.
 </para>
________________________________________________________-->
 <para>
  与B树一样，哈希索引执行简单的索引元组删除。这是一个延迟维护操作，用于删除已知可以安全删除的索引元组（其项标识符的LP_DEAD位已设置的那些）。如果insert发现页面上没有可用空间，我们会尝试通过删除死索引元组来避免创建新的溢出页面。如果此时页面已被锁定，则无法删除。VACUUM期间也会删除死索引指针。
 </para>
<!-- pgdoc-cn_end sig_en=beac1daac65e217d08ab101c357863d2 -->

<!-- pgdoc-cn_start sig_en=c3b3ddd37549e3d65d33d58ee356c343 sig_cn_org=3bfc4418827c9cd77e124351badcafbb source=15.7 
 <para>
  If it can, VACUUM will also try to squeeze the index tuples onto as
  few overflow pages as possible, minimizing the overflow chain. If an
  overflow page becomes empty, overflow pages can be recycled for reuse
  in other buckets, though we never return them to the operating system.
  There is currently no provision to shrink a hash index, other than by
  rebuilding it with REINDEX.
  There is no provision for reducing the number of buckets, either.
 </para>
________________________________________________________-->
 <para>
  如果可能的话，VACUUM还会尝试将索引元组挤压到尽可能少的溢出页上，最小化溢出链。
  如果一个溢出页变为空，溢出页可以被回收重用在其他桶中，尽管我们从不将它们返回给操作系统。
  目前没有任何方法来缩小哈希索引，除非通过使用REINDEX重新构建它。
  也没有减少桶的数量的方法。
</para>
<!-- pgdoc-cn_end sig_en=c3b3ddd37549e3d65d33d58ee356c343 -->

<!-- pgdoc-cn_start sig_en=9c25f0e32265533295ef8a264ef0c964 sig_cn_org=None source=14.1 
 <para>
  Hash indexes may expand the number of bucket pages as the number of
  rows indexed grows. The hash key-to-bucket-number mapping is chosen so that
  the index can be incrementally expanded. When a new bucket is to be added to
  the index, exactly one existing bucket will need to be "split", with some of
  its tuples being transferred to the new bucket according to the updated
  key-to-bucket-number mapping.
 </para>
________________________________________________________-->
 <para>
  哈希索引可能会随着索引行数的增加而扩展桶页数。选择哈希键到桶号的映射，以便可以增量扩展索引。当一个新的桶要添加到索引中时，只需要“拆分”一个现有的桶，其中的一些元组将根据更新后的key到桶号的映射转移到新桶。
 </para>
<!-- pgdoc-cn_end sig_en=9c25f0e32265533295ef8a264ef0c964 -->

<!-- pgdoc-cn_start sig_en=3bde8a053d5ae32471f14b8d7ae1fca2 sig_cn_org=None source=14.1 
 <para>
  The expansion occurs in the foreground, which could increase execution
  time for user inserts. Thus, hash indexes may not be suitable for tables
  with rapidly increasing number of rows.
 </para>
________________________________________________________-->
 <para>
  扩展发生在前端，这可能会增加用户插入的执行时间。因此，哈希索引可能不适用于行数快速增加的表。
 </para>
<!-- pgdoc-cn_end sig_en=3bde8a053d5ae32471f14b8d7ae1fca2 -->

</sect1>

<sect1 id="hash-implementation">
<!-- pgdoc-cn_start sig_en=05afc0ba076ec1f3b376592bf28077e0 sig_cn_org=None source=14.1 
 <title>Implementation</title>
________________________________________________________-->
 <title>实现</title>
<!-- pgdoc-cn_end sig_en=05afc0ba076ec1f3b376592bf28077e0 -->

<!-- pgdoc-cn_start sig_en=7fe8f96aab9a97440e948ac98d2a3559 sig_cn_org=None source=14.1 
 <para>
  There are four kinds of pages in a hash index: the meta page (page zero),
  which contains statically allocated control information; primary bucket
  pages; overflow pages; and bitmap pages, which keep track of overflow
  pages that have been freed and are available for re-use. For addressing
  purposes, bitmap pages are regarded as a subset of the overflow pages.
 </para>
________________________________________________________-->
 <para>
  哈希索引中有四种页面：元页面（零页），其中包含静态分配的控制信息；主桶页；溢出页；和位图页，它们跟踪已释放并可供重用的溢出页。出于寻址目的，位图页被视为溢出页的子集。
 </para>
<!-- pgdoc-cn_end sig_en=7fe8f96aab9a97440e948ac98d2a3559 -->

<!-- pgdoc-cn_start sig_en=741235c95f45d7b2fea6bd1ac46af813 sig_cn_org=None source=14.1 
 <para>
  Both scanning the index and inserting tuples require locating the bucket
  where a given tuple ought to be located. To do this, we need the bucket
  count, highmask, and lowmask from the metapage; however, it's undesirable
  for performance reasons to have to have to lock and pin the metapage for
  every such operation. Instead, we retain a cached copy of the metapage
  in each backend's relcache entry. This will produce the correct bucket
  mapping as long as the target bucket hasn't been split since the last
  cache refresh.
 </para>
________________________________________________________-->
 <para>
  扫描索引和插入元组都需要定位给定元组应该位于的桶。为此，我们需要元页面中的桶计数、highmask和lowmask；然而，出于性能原因，必须为每个此类操作加锁和锁定元页是不可取的。相反，我们在每个后端的relcache条目中保留了元页面的缓存副本。只要目标桶自上次缓存刷新后未被拆分，这将生成正确的桶映射。
 </para>
<!-- pgdoc-cn_end sig_en=741235c95f45d7b2fea6bd1ac46af813 -->

<!-- pgdoc-cn_start sig_en=f8c391e8cad42b1c74af1c10a8c5bd55 sig_cn_org=None source=14.1 
 <para>
  Primary bucket pages and overflow pages are allocated independently since
  any given index might need more or fewer overflow pages relative to its
  number of buckets. The hash code uses an interesting set of addressing
  rules to support a variable number of overflow pages while not having to
  move primary bucket pages around after they are created.
 </para>
________________________________________________________-->
 <para>
  主桶页和溢出页是独立分配的，因为任何给定的索引相对于其桶的数量可能需要更多或更少的溢出页。哈希代码使用一组有趣的寻址规则来支持可变数量的溢出页，而不必在创建主桶页后四处移动。
 </para>
<!-- pgdoc-cn_end sig_en=f8c391e8cad42b1c74af1c10a8c5bd55 -->

<!-- pgdoc-cn_start sig_en=97f668238d0ff637616082082410d16e sig_cn_org=None source=14.1 
 <para>
  Each row in the table indexed is represented by a single index tuple in
  the hash index. Hash index tuples are stored in bucket pages, and if
  they exist, overflow pages. We speed up searches by keeping the index entries
  in any one index page sorted by hash code, thus allowing binary search to be
  used within an index page. Note however that there is *no* assumption about
  the relative ordering of hash codes across different index pages of a bucket.
 </para>
________________________________________________________-->
 <para>
  索引表中的每一行由哈希索引中的单个索引元组表示。哈希索引元组存储在桶页中，如果存在，则存储在溢出页中。我们通过将任意一个索引页中的索引项按哈希代码排序来加快搜索速度，从而允许在索引页中使用二进制搜索。但是，请注意，对于一个桶的不同索引页上的哈希代码的相对排序，没有上述排序。
 </para>
<!-- pgdoc-cn_end sig_en=97f668238d0ff637616082082410d16e -->

<!-- pgdoc-cn_start sig_en=451f46b57f937787a1bd180087883308 sig_cn_org=None source=14.1 
 <para>
  The bucket splitting algorithms to expand the hash index are too complex to
  be worthy of mention here, though are described in more detail in
  <filename>src/backend/access/hash/README</filename>.
  The split algorithm is crash safe and can be restarted if not completed
  successfully.
 </para>
________________________________________________________-->
 <para>
  用于扩展哈希索引的桶分割算法过于复杂，不值得在此提及，下面有更详细的描述<filename>src/backend/access/hash/README</filename>。拆分算法是崩溃安全的，如果未成功完成，可以重新启动。
 </para>
<!-- pgdoc-cn_end sig_en=451f46b57f937787a1bd180087883308 -->

</sect1>

</chapter>
